However, increasing bins slows down the code a lot and we don't know the convergence properties for every parameter set. So a more robust solution of the problem would be good, one which also conserves mass at least approximately. 

We had three ideas of how to do this - Jordan will send an email later today describing these methods - related to adaptive binning and spline interpolation. I describe my suggestion below. The idea with this email and Jordan's is to get your feedback, thoughts on the ideas and which (modified or combined) should we try implementing. Feel free to talk to Jordan about this sometime next week - I can catch up later. 

My suggestion is that we effectively get rid of rebinning. In particular, we require rebinning in the code because the bin points are used to calculate the St no, relative particle velocity, and calculate the destruction timescale and dm/dt (growth rate). Rebinning is need to keep the number of particle sizes tractable for these calculations at next time step. However, if we have two grids simultaneously - a coarser grid with say 500 points and a finer grid with actual particle sizes, then After each time step, the size of the particle is not changed. The coarser grid is used to calculate St, etc at each time step and the actual destruction or dm/dt is calculated for the particles in finer grid using interpolation (linear should be fine) from the coarser grid. Although this introduces some small error, I think it should not matter given that some other parts of the model are also not perfect like the details of growth prescription. The nice part of this suggestion is that it should ensure particle growth is correctly captured, even small incremental growth. In practice, some minor amount of rebinning would need to be done since otherwise the finer grid size would increase very rapidly. However, something simple like rounding off to first or second decimal places might be enough (will need to play with this). The mass change in the round off can be kept track of to ensure mass conservation. Also, Implementing this in the code should be relatively straightforward. 


Another two ideas we briefly thought about was to try to avoid binning and to try to use splines that we fit between some control points (or some other method, I guess we don't require differentiability at all). But then, instead of just having discrete bins, we have some N(x) and N(x+1) with a mass associated with the two. We then have a function that describes the distribution in here, that we can draw from. I think with some clever thought, perhaps we can avoid most of the issues from binning by making it entirely distribution based. There would still be some issues that would be similiar to the ones faced when binning (for example, incremental growth might still cause some issues) but I think they'd be much less.

Another idea, somewhat similiar to Tushar's above, would be to use something like a multigrid approach. We run everything on a coarse grid and a fine grid. Then, we compute the difference. For ever region that does not agree on the two grids to within some predefined tolerance, we then double the bin resolution and repeat. So, some regions would be successfully calculated on a coarse grid and others would be calculated on a finer grid. 

As Tushar pointed out, I think that there are two things we want to keep in mind:
We want the code to be relatively fast so that we can explore a wide range of parameters
We want to be able to say something about relative times different conditions reach certain sizes. Thus, I think we want to be sure we are relatively confident in the time scales.

